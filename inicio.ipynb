{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1381b72",
   "metadata": {},
   "source": [
    "¿Qué Es el Web Scraping?\n",
    "El web scraping es un conjunto de prácticas utilizadas para extraer automáticamente — o «scrapear» — datos de la web.\n",
    "\n",
    "El web scraping se refiere al proceso de extracción de contenidos y datos de sitios web mediante software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202bdac",
   "metadata": {},
   "source": [
    "Este script realiza web scraping de la página de un libro específico en el sitio \"Books to Scrape\".\n",
    "\n",
    "Primero, hacemos una petición HTTP con `requests.get(url)` para obtener el contenido HTML de la página.\n",
    "\n",
    "Luego, usamos `BeautifulSoup(response.text, 'html.parser')` para convertir ese HTML crudo en un objeto navegable.\n",
    "Esto permite buscar elementos fácilmente usando métodos como `.find()`, `.select_one()`, etc.\n",
    "\n",
    "Extraemos el título con `soup.find('h1').text`, el precio con `soup.select_one('.price_color').text`,  y lo limpiamos con `.replace()` para quitar caracteres no deseados (como \"Â\").\n",
    "\n",
    "Para la disponibilidad, usamos `.select_one('.availability').text.strip()` para eliminar espacios en blanco.\n",
    "\n",
    "La descripción del libro está justo después del div con id `product_description`, por eso usamos `.find_next_sibling('p')`.\n",
    "\n",
    "Finalmente, las estrellas están codificadas como una clase CSS en la etiqueta <p>, como por ejemplo: `<p class=\"star-rating Three\">.`\n",
    "\n",
    "Al usar `tag['class'][1]`, obtenemos el texto que indica la cantidad de estrellas (por ejemplo, \"Three\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db650b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: A Light in the Attic\n",
      "Precio: £51.77\n",
      "Disponibilidad: In stock (22 available)\n",
      "Descripción: It's hard to imagine a world without A Light in the Attic. T...\n",
      "Estrellas: Three\n"
     ]
    }
   ],
   "source": [
    "# Extraer la información de un solo libro\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup #Averiguar bien\n",
    "\n",
    "# URL del libro\n",
    "url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "\n",
    "# Hacemos la petición\n",
    "response = requests.get(url)\n",
    "\n",
    "# BeautifulSoup(response.text, 'html.parser') transforma el HTML crudo en un objeto navegable.\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Obtenemos el título\n",
    "title = soup.find('h1').text\n",
    "\n",
    "# .text te devuelve el texto visible dentro de una etiqueta HTML.\n",
    "# La gente usa .text sin saber qué devuelve la etiqueta. Si la etiqueta no tiene texto visible (ej: <img> o <meta>), .text te da vacío.\n",
    "\n",
    "# Obtenemos precio\n",
    "price = soup.select_one('.price_color').text\n",
    "price = price.replace('Â', '')  # Limpia cualquier carácter raro que aparezca\n",
    "\n",
    "# Obtenemos disponibilidad\n",
    "availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "# Descripción (viene en el siguiente <p> después del div con id 'product_description')\n",
    "desc_tag = soup.find('div', id='product_description')\n",
    "# Usamos `.find_next_sibling('p')`, que busca el **próximo hermano (sibling)** que sea una etiqueta <p>.\n",
    "description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripción\"\n",
    "\n",
    "# Las estrellas están codificadas en la clase CSS de un <p>\n",
    "# Ese guion bajo es una convención para evitar conflicto. Internamente, BeautifulSoup ya sabe que class_ se refiere al atributo class del HTML\n",
    "# Al hacer tag['class'], obtenemos una lista como ['star-rating', 'Three'], y el índice [1] nos da la cantidad de estrellas en texto.\n",
    "star_tag = soup.find('p', class_='star-rating')\n",
    "stars = star_tag['class'][1]\n",
    "\n",
    "# Imprimimos\n",
    "print(f\"Título: {title}\")\n",
    "print(f\"Precio: {price}\")\n",
    "print(f\"Disponibilidad: {availability}\")\n",
    "print(f\"Descripción: {description[:60]}...\")\n",
    "print(f\"Estrellas: {stars}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c25ea1",
   "metadata": {},
   "source": [
    "Scrapeo de varias páginas\n",
    "\n",
    "Que hacen:\n",
    "\n",
    "1. requests.get(url) hace una petición HTTP a la URL y devuelve la respuesta con el HTML.\n",
    "\n",
    "2. BeautifulSoup transforma el texto HTML en un objeto para buscar etiquetas fácilmente.\n",
    "\n",
    "3. .find() busca la primera etiqueta que cumple la condición.\n",
    "\n",
    "4. .select() busca todas las etiquetas que cumplen un selector CSS (devuelve lista).\n",
    "\n",
    "5. .select_one() busca el primer elemento que cumple el selector CSS (devuelve un tag).\n",
    "\n",
    "6. En las URLs, a veces hay rutas relativas con \"../\", hay que limpiarlas para crear URLs absolutas.\n",
    "\n",
    "7. time.sleep(0.1) hace que el programa espere 0.1 segundos para no saturar el servidor y evitar bloqueos.\n",
    "\n",
    "8. El bucle while True se usa para seguir scrapeando páginas hasta que no haya más páginas siguientes.\n",
    "\n",
    "9. Las estrellas del libro están en la clase CSS 'star-rating X', donde X es la cantidad en texto (One, Two, Three, etc).\n",
    "\n",
    "10. Guardamos los datos en una lista de diccionarios para luego exportarlos o procesarlos fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3859dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-last-mile-amos-decker-2_754/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-last-mile-amos-decker-2_754/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d547080>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d9bbce0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/hide-away-eve-duncan-20_620/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/hide-away-eve-duncan-20_620/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d8b9190>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-widow_609/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-widow_609/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d60af60>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html\n",
      "Total de libros scrapeados: 32\n",
      "[{'Titulo': 'Sharp Objects', 'Precio': '£47.82', 'Disponibilidad': 'In stock (20 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html'}, {'Titulo': 'In a Dark, Dark Wood', 'Precio': '£19.63', 'Disponibilidad': 'In stock (18 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html'}, {'Titulo': 'The Past Never Ends', 'Precio': '£56.50', 'Disponibilidad': 'In stock (16 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html'}, {'Titulo': 'A Murder in Time', 'Precio': '£16.64', 'Disponibilidad': 'In stock (16 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/a-murder-in-time_877/index.html'}, {'Titulo': 'The Murder of Roger Ackroyd (Hercule Poirot #4)', 'Precio': '£44.10', 'Disponibilidad': 'In stock (15 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html'}, None, {'Titulo': 'That Darkness (Gardiner and Renner #1)', 'Precio': '£13.92', 'Disponibilidad': 'In stock (14 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/that-darkness-gardiner-and-renner-1_743/index.html'}, {'Titulo': 'Tastes Like Fear (DI Marnie Rome #3)', 'Precio': '£10.69', 'Disponibilidad': 'In stock (14 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/tastes-like-fear-di-marnie-rome-3_742/index.html'}, {'Titulo': 'A Time of Torment (Charlie Parker #14)', 'Precio': '£48.35', 'Disponibilidad': 'In stock (14 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/a-time-of-torment-charlie-parker-14_657/index.html'}, None, {'Titulo': 'Poisonous (Max Revere Novels #3)', 'Precio': '£26.80', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/poisonous-max-revere-novels-3_627/index.html'}, {'Titulo': 'Murder at the 42nd Street Library (Raymond Ambler #1)', 'Precio': '£54.36', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/murder-at-the-42nd-street-library-raymond-ambler-1_624/index.html'}, {'Titulo': 'Most Wanted', 'Precio': '£35.28', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/most-wanted_623/index.html'}, None, {'Titulo': 'Boar Island (Anna Pigeon #19)', 'Precio': '£59.48', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/boar-island-anna-pigeon-19_613/index.html'}, None, {'Titulo': 'Playing with Fire', 'Precio': '£13.71', 'Disponibilidad': 'In stock (11 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/playing-with-fire_602/index.html'}, {'Titulo': 'What Happened on Beale Street (Secrets of the South Mysteries #2)', 'Precio': '£25.37', 'Disponibilidad': 'In stock (7 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html'}, {'Titulo': \"The Bachelor Girl's Guide to Murder (Herringford and Watts Mysteries #1)\", 'Precio': '£52.30', 'Disponibilidad': 'In stock (7 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/the-bachelor-girls-guide-to-murder-herringford-and-watts-mysteries-1_491/index.html'}, {'Titulo': 'Delivering the Truth (Quaker Midwife Mystery #1)', 'Precio': '£20.89', 'Disponibilidad': 'In stock (7 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/delivering-the-truth-quaker-midwife-mystery-1_464/index.html'}, {'Titulo': 'The Mysterious Affair at Styles (Hercule Poirot #1)', 'Precio': '£24.80', 'Disponibilidad': 'In stock (6 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-mysterious-affair-at-styles-hercule-poirot-1_452/index.html'}, {'Titulo': 'In the Woods (Dublin Murder Squad #1)', 'Precio': '£38.38', 'Disponibilidad': 'In stock (6 available)', 'Estrellas': 'Two', 'URL': 'https://books.toscrape.com/catalogue/in-the-woods-dublin-murder-squad-1_433/index.html'}, {'Titulo': 'The Silkworm (Cormoran Strike #2)', 'Precio': '£23.05', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/the-silkworm-cormoran-strike-2_280/index.html'}, {'Titulo': 'The Exiled', 'Precio': '£43.45', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/the-exiled_247/index.html'}, {'Titulo': \"The Cuckoo's Calling (Cormoran Strike #1)\", 'Precio': '£19.21', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/the-cuckoos-calling-cormoran-strike-1_239/index.html'}, {'Titulo': 'Extreme Prey (Lucas Davenport #26)', 'Precio': '£25.40', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/extreme-prey-lucas-davenport-26_154/index.html'}, {'Titulo': 'Career of Evil (Cormoran Strike #3)', 'Precio': '£24.72', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Two', 'URL': 'https://books.toscrape.com/catalogue/career-of-evil-cormoran-strike-3_137/index.html'}, {'Titulo': \"The No. 1 Ladies' Detective Agency (No. 1 Ladies' Detective Agency #1)\", 'Precio': '£57.70', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-no-1-ladies-detective-agency-no-1-ladies-detective-agency-1_76/index.html'}, {'Titulo': 'The Girl You Lost', 'Precio': '£12.29', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/the-girl-you-lost_66/index.html'}, {'Titulo': 'The Girl In The Ice (DCI Erika Foster #1)', 'Precio': '£15.85', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/the-girl-in-the-ice-dci-erika-foster-1_65/index.html'}, {'Titulo': 'Blood Defense (Samantha Brinkman #1)', 'Precio': '£20.30', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/blood-defense-samantha-brinkman-1_8/index.html'}, {'Titulo': \"1st to Die (Women's Murder Club #1)\", 'Precio': '£53.98', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/1st-to-die-womens-murder-club-1_2/index.html'}]\n"
     ]
    }
   ],
   "source": [
    "# Extraer la información de una categoría\n",
    "\n",
    "import requests # Para hacer peticiones HTTPS y obtener el contenido de las páginas\n",
    "from bs4 import BeautifulSoup # Para parsear (analizar) el contenido de la página web\n",
    "import time # Para pausar entre peticiones y no sobrecargar el servidor\n",
    "\n",
    "base_url = 'https://books.toscrape.com/catalogue/'\n",
    "\n",
    "category_url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
    "\n",
    "# Lista donde se guardaran los datos de todos los libros\n",
    "all_books = []\n",
    "\n",
    "# Función scrape_book, toma la url de un libro especifico y devuelve los datos scrapeados en forma de diccionario\n",
    "def scrape_book(url):\n",
    "\n",
    "  # Hacemos la petición\n",
    "  try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "  except requests.RequestException as e:\n",
    "    print(f\"❌ Error al acceder a {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "  # BeautifulSoup(response.text, 'html.parser') transforma el HTML crudo en un objeto navegable.\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Obtenemos el título\n",
    "  title = soup.find('h1').text\n",
    "\n",
    "  # Obtenemos precio\n",
    "  price = soup.select_one('.price_color').text\n",
    "  price = price.replace('Â', '')  # Limpia cualquier carácter raro que aparezca\n",
    "\n",
    "  # Obtenemos disponibilidad\n",
    "  availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "  # Descripción (viene en el siguiente <p> después del div con id 'product_description')\n",
    "  desc_tag = soup.find('div', id='product_description')\n",
    "  # Usamos `.find_next_sibling('p')`, que busca el **próximo hermano (sibling)** que sea una etiqueta <p>.\n",
    "  description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripción\"\n",
    "\n",
    "  # Las estrellas están codificadas en la clase CSS de un <p>\n",
    "  # Ese guion bajo es una convención para evitar conflicto. Internamente, BeautifulSoup ya sabe que class_ se refiere al atributo class del HTML\n",
    "  # Al hacer tag['class'], obtenemos una lista como ['star-rating', 'Three'], y el índice [1] nos da la cantidad de estrellas en texto.\n",
    "  star_tag = soup.find('p', class_='star-rating')\n",
    "  stars = star_tag['class'][1]\n",
    "\n",
    "  return {\n",
    "    'Titulo': title,\n",
    "    'Precio': price,\n",
    "    'Disponibilidad': availability,\n",
    "    'Estrellas': stars,\n",
    "    'URL': url\n",
    "  }\n",
    "\n",
    "# Funcion scrape_category, toma la url de una categoria, recorre todas sus paginas y scrapea todos los libros que encuentra\n",
    "def scrape_category(url):\n",
    "  while True:\n",
    "    print(f\"Scrapeando pagina: {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Seleccionamos todos los libros en la pagina actual de esa categoría\n",
    "    book_articles = soup.select('article.product_pod')\n",
    "\n",
    "    for book in book_articles:\n",
    "\n",
    "      # Obtenemos el href del libro\n",
    "      relative_url = book.find('h3').find('a')['href']\n",
    "\n",
    "      # relative_url trae la url completa, entonces la limpiamos para después concatenarla a base_url\n",
    "      book_url = base_url + relative_url.replace('../../../', '')\n",
    "\n",
    "      # Scrapeamos el libro\n",
    "      book_data = scrape_book(book_url)\n",
    "\n",
    "      # Guardamos el resultado\n",
    "      all_books.append(book_data)\n",
    "\n",
    "      # Pausa para evitar sobrecarga\n",
    "      time.sleep(10)\n",
    "\n",
    "    # Verificamos si hay un boton next para seguir scrapeando la sgte pagina\n",
    "    # Usa un selector CSS (li.next > a) para encontrar el link de la siguiente página. Si existe: next_button va a tener un tag <a>.\n",
    "    next_button = soup.select_one('li.next > a')\n",
    "    if next_button:\n",
    "      next_page_url = next_button['href']\n",
    "      # rsplit(separador, cantidad) divide un string desde el final, y solo hace la cantidad de cortes que vos le indiques.\n",
    "      url = url.rsplit('/', 1)[0] + '/' + next_page_url\n",
    "    else:\n",
    "      break\n",
    "\n",
    "scrape_category(category_url)\n",
    "\n",
    "print(f\"Total de libros scrapeados: {len(all_books)}\")\n",
    "print(all_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad444aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Se encontraron 50 categorias. \n",
      "\n",
      "\n",
      " Scrapeando categoria: Travel -> https://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "\n",
      " Scrapeando categoria: Mystery -> https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-past-never-ends_942/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d5832f0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc20710>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-last-mile-amos-decker-2_754/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-last-mile-amos-decker-2_754/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853eb880b0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc202f0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/most-wanted_623/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/most-wanted_623/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dd50b90>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853ebc9910>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-bachelor-girls-guide-to-murder-herringford-and-watts-mysteries-1_491/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-bachelor-girls-guide-to-murder-herringford-and-watts-mysteries-1_491/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc468a0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/delivering-the-truth-quaker-midwife-mystery-1_464/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/delivering-the-truth-quaker-midwife-mystery-1_464/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc474a0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/in-the-woods-dublin-murder-squad-1_433/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/in-the-woods-dublin-murder-squad-1_433/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d582ab0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-exiled_247/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-exiled_247/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853e07e9c0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-cuckoos-calling-cormoran-strike-1_239/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-cuckoos-calling-cormoran-strike-1_239/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dd502c0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/extreme-prey-lucas-davenport-26_154/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/extreme-prey-lucas-davenport-26_154/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853e07f320>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "\n",
      " Scrapeando categoria: Historical Fiction -> https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html\n",
      "❌ Error al acceder a https://books.toscrape.com/catalogue/the-constant-princess-the-tudor-court-1_493/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-constant-princess-the-tudor-court-1_493/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d92bb30>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Scrapeando categoria: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory.text.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m     scrape_category(category_url)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mscrape_all_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal de libros scrapeados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_books)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_books)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mscrape_all_categories\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    107\u001b[39m category_url = base_site + relative_url\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Scrapeando categoria: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory.text.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43mscrape_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mscrape_category\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     77\u001b[39m   all_books.append(book_data)\n\u001b[32m     79\u001b[39m   \u001b[38;5;66;03m# Pausa para evitar sobrecarga\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m   \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Verificamos si hay un boton next para seguir scrapeando la sgte pagina\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Usa un selector CSS (li.next > a) para encontrar el link de la siguiente página. Si existe: next_button va a tener un tag <a>.\u001b[39;00m\n\u001b[32m     84\u001b[39m next_button = soup.select_one(\u001b[33m'\u001b[39m\u001b[33mli.next > a\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Extraer información de toda la página\n",
    "\n",
    "import requests # Para hacer peticiones HTTPS y obtener el contenido de las páginas\n",
    "from bs4 import BeautifulSoup # Para parsear (analizar) el contenido de la página web\n",
    "import time # Para pausar entre peticiones y no sobrecargar el servidor\n",
    "\n",
    "base_url = 'https://books.toscrape.com/catalogue/'\n",
    "\n",
    "# Lista donde se guardaran los datos de todos los libros\n",
    "all_books = []\n",
    "\n",
    "# Función scrape_book, toma la url de un libro especifico y devuelve los datos scrapeados en forma de diccionario\n",
    "def scrape_book(url):\n",
    "\n",
    "  # Hacemos la petición\n",
    "  try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "  except requests.RequestException as e:\n",
    "    print(f\"❌ Error al acceder a {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "  # BeautifulSoup(response.text, 'html.parser') transforma el HTML crudo en un objeto navegable.\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Obtenemos el título\n",
    "  title = soup.find('h1').text\n",
    "\n",
    "  # Obtenemos precio\n",
    "  price = soup.select_one('.price_color').text\n",
    "  price = price.replace('Â', '')  # Limpia cualquier carácter raro que aparezca\n",
    "\n",
    "  # Obtenemos disponibilidad\n",
    "  availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "  # Descripción (viene en el siguiente <p> después del div con id 'product_description')\n",
    "  desc_tag = soup.find('div', id='product_description')\n",
    "  # Usamos `.find_next_sibling('p')`, que busca el **próximo hermano (sibling)** que sea una etiqueta <p>.\n",
    "  description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripción\"\n",
    "\n",
    "  # Las estrellas están codificadas en la clase CSS de un <p>\n",
    "  # Ese guion bajo es una convención para evitar conflicto. Internamente, BeautifulSoup ya sabe que class_ se refiere al atributo class del HTML\n",
    "  # Al hacer tag['class'], obtenemos una lista como ['star-rating', 'Three'], y el índice [1] nos da la cantidad de estrellas en texto.\n",
    "  star_tag = soup.find('p', class_='star-rating')\n",
    "  stars = star_tag['class'][1]\n",
    "\n",
    "  return {\n",
    "    'Titulo': title,\n",
    "    'Precio': price,\n",
    "    'Disponibilidad': availability,\n",
    "    'Estrellas': stars,\n",
    "    'URL': url\n",
    "  }\n",
    "\n",
    "# Funcion scrape_category, toma la url de una categoria, recorre todas sus paginas y scrapea todos los libros que encuentra\n",
    "def scrape_category(url):\n",
    "  while True:\n",
    "    print(f\"Scrapeando pagina: {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Seleccionamos todos los libros en la pagina actual de esa categoría\n",
    "    book_articles = soup.select('article.product_pod')\n",
    "\n",
    "    for book in book_articles:\n",
    "\n",
    "      # Obtenemos el href del libro\n",
    "      relative_url = book.find('h3').find('a')['href']\n",
    "\n",
    "      # relative_url trae la url completa, entonces la limpiamos para después concatenarla a base_url\n",
    "      book_url = base_url + relative_url.replace('../../../', '')\n",
    "\n",
    "      # Scrapeamos el libro\n",
    "      book_data = scrape_book(book_url)\n",
    "\n",
    "      # Guardamos el resultado\n",
    "      all_books.append(book_data)\n",
    "\n",
    "      # Pausa para evitar sobrecarga\n",
    "      time.sleep(10)\n",
    "\n",
    "    # Verificamos si hay un boton next para seguir scrapeando la sgte pagina\n",
    "    # Usa un selector CSS (li.next > a) para encontrar el link de la siguiente página. Si existe: next_button va a tener un tag <a>.\n",
    "    next_button = soup.select_one('li.next > a')\n",
    "    if next_button:\n",
    "      next_page_url = next_button['href']\n",
    "      # rsplit(separador, cantidad) divide un string desde el final, y solo hace la cantidad de cortes que vos le indiques.\n",
    "      url = url.rsplit('/', 1)[0] + '/' + next_page_url\n",
    "    else:\n",
    "      break\n",
    "\n",
    "def scrape_all_categories():\n",
    "\n",
    "  home_url = 'https://books.toscrape.com/index.html'\n",
    "  base_site = 'https://books.toscrape.com/'\n",
    "\n",
    "  response = requests.get(home_url)\n",
    "  response.raise_for_status()\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  category_links = soup.select('div.side_categories ul li ul li a')\n",
    "\n",
    "  print(f\"\\n Se encontraron {len(category_links)} categorias. \\n\")\n",
    "\n",
    "  for category in category_links:\n",
    "    relative_url = category['href']\n",
    "    category_url = base_site + relative_url\n",
    "\n",
    "    print(f\"\\n Scrapeando categoria: {category.text.strip()} -> {category_url}\")\n",
    "\n",
    "    scrape_category(category_url)\n",
    "\n",
    "scrape_all_categories()\n",
    "\n",
    "print(f\"Total de libros scrapeados: {len(all_books)}\")\n",
    "print(all_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842091e",
   "metadata": {},
   "source": [
    "Para poder pasar los datos a un CSV\n",
    "\n",
    "Un archivo CSV (valores separados por comas) es un archivo de texto que tiene un formato específico que permite guardar los datos en un formato de tabla estructurada.\n",
    "\n",
    "with ... as f: 👉 Context manager: abre y cierra el archivo solo, incluso si explota algo. f es el manejador.\n",
    "\n",
    "open(path, ...) 👉 ruta del archivo. Si no existe la carpeta, revienta con FileNotFoundError.\n",
    "\n",
    "'w' 👉 write: crea el archivo o sobrescribe si ya existe.\n",
    "\n",
    "newline='' 👉 especial para CSV en Windows: evita que se inserten líneas en blanco entre filas.\n",
    "\n",
    "encoding='utf-8' 👉 guarda acentos/ñ/emoji sin romperse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapeando página: https://books.toscrape.com/catalogue/page-1.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-2.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-3.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-4.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-5.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-6.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-7.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-8.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-9.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-10.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-11.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-12.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-13.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-14.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-15.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-16.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-17.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-18.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-19.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-20.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-21.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-22.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-23.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-24.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-25.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-26.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-27.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-28.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-29.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-30.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-31.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-32.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-33.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-34.html\n",
      "Scrapeando página: https://books.toscrape.com/catalogue/page-35.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import csv\n",
    "\n",
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "CATALOGUE_URL = BASE_URL + \"catalogue/\"\n",
    "\n",
    "all_books = []\n",
    "\n",
    "def fetch_author(title):\n",
    "    try:\n",
    "        url  = f\"https://openlibrary.org/search.json?title={requests.utils.quote(title)}&limit=1\"\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "        docs = resp.json().get('docs', [])\n",
    "        if docs and docs[0].get('author_name'):\n",
    "            return docs[0]['author_name'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error al buscar autor para «{title}»: {e}\")\n",
    "    return \"Desconocido\"\n",
    "\n",
    "def scrape_book(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌ Error al acceder a {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').text\n",
    "    author = fetch_author(title)\n",
    "    price = soup.select_one('.price_color').text.replace('Â', '')\n",
    "    availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "    desc_tag = soup.find('div', id='product_description')\n",
    "    description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripción\"\n",
    "\n",
    "    star_tag = soup.find('p', class_='star-rating')\n",
    "    stars = star_tag['class'][1] if star_tag else \"No rating\"\n",
    "\n",
    "    return {\n",
    "        'Titulo': title,\n",
    "        'Autor': author,\n",
    "        'Precio': price,\n",
    "        'Disponibilidad': availability,\n",
    "        'Estrellas': stars,\n",
    "        'Descripcion': description,\n",
    "        'URL': url\n",
    "    }\n",
    "\n",
    "def scrape_all_books():\n",
    "    url = CATALOGUE_URL + \"page-1.html\"\n",
    "    while True:\n",
    "        print(f\"Scrapeando página: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        books = soup.select('article.product_pod')\n",
    "        book_urls = []\n",
    "\n",
    "        for b in books:\n",
    "          # Encontramos el href del <a> dentro del <h3>\n",
    "          relative_url = b.find('h3').a['href']\n",
    "\n",
    "          # Limpiamos esa url\n",
    "          cleaned_url = relative_url.replace('../../../', '')\n",
    "\n",
    "          full_url = CATALOGUE_URL + cleaned_url\n",
    "\n",
    "          book_urls.append(full_url)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers = 15) as executor:\n",
    "          results = executor.map(scrape_book, book_urls)\n",
    "          for book_data in  results:\n",
    "            if book_data:\n",
    "              all_books.append(book_data)\n",
    "\n",
    "        # Verificamos si hay un boton next para seguir scrapeando la sgte pagina\n",
    "        # Usa un selector CSS (li.next > a) para encontrar el link de la siguiente página. Si existe: next_button va a tener un tag <a>.\n",
    "        next_button = soup.select_one('li.next > a')\n",
    "        if next_button:\n",
    "          next_page = next_button['href']\n",
    "          # rsplit(separador, cantidad) divide un string desde el final, y solo hace la cantidad de cortes que vos le indiques.\n",
    "          url = url.rsplit('/', 1)[0] + '/' + next_page\n",
    "        else:\n",
    "          break\n",
    "\n",
    "def export_books_to_csv(rows, path='books.csv'):\n",
    "    \"\"\"\n",
    "    Guarda una lista de dicts (all_books) en un CSV.\n",
    "    - Usa las claves del primer dict como encabezados.\n",
    "    - Codifica en UTF-8.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        print(\"⚠️ Nada para guardar (lista vacía).\")\n",
    "        return\n",
    "    # escribir encabezados y filas\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
    "        headers = list(rows[0].keys())   # columnas: Titulo, Autor, Precio, etc.\n",
    "        w = csv.DictWriter(f, fieldnames=headers)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "    print(f\"✅ CSV exportado: {path} ({len(rows)} filas)\")\n",
    "\n",
    "\n",
    "scrape_all_books()\n",
    "\n",
    "print(f\"\\n Total de libros scrapeados: {len(all_books)}\")\n",
    "\n",
    "export_books_to_csv(all_books)  # crea books.csv en tu carpeta actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Conectar la base de datos local 'scraping.db'\n",
    "conector = sqlite3.connect('books.db')\n",
    "cursor = conector.cursor()\n",
    "\n",
    "# Crear tabla autores\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS authors (\n",
    "               id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               name TEXT UNIQUE\n",
    "               );\n",
    "\"\"\")\n",
    "\n",
    "# Crear tabla de libros\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS books (\n",
    "               id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               title TEXT,\n",
    "               price REAL,\n",
    "               availability TEXT,\n",
    "               stars INTEGER,\n",
    "               description TEXT,\n",
    "               url TEXT UNIQUE\n",
    "               );\n",
    "\"\"\")\n",
    "\n",
    "# Creamos la tabla intermedia para lograr la relación de muchos a muchos\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS books_authors(\n",
    "               book_id INTEGER,\n",
    "               author_id integer,\n",
    "               PRIMARY KEY (book_id, author_id),\n",
    "               FOREIGN KEY (book_id) REFERENCES books(id),\n",
    "               FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "               )\n",
    "\"\"\")\n",
    "\n",
    "# Confirmamos la creacion de las tablas\n",
    "conector.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d63fc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# 🔁 ABRIR NUEVA CONEXIÓN + CURSOR (si cerraste antes, hay que recrearlos)\n",
    "conector = sqlite3.connect('books.db')\n",
    "conector.execute(\"PRAGMA foreign_keys = ON;\")  # que se apliquen las FKs\n",
    "cursor = conector.cursor()\n",
    "\n",
    "# Insertamos datos dentro de las tablas\n",
    "# Recorremos el diccionario y vamos guardando los datos en variables\n",
    "for book in all_books:\n",
    "  title = book['Titulo']\n",
    "  author_name = book['Autor']\n",
    "  price = float(book['Precio'].lstrip('£')) # Convertimos de texto a numero\n",
    "  availability = book['Disponibilidad']\n",
    "\n",
    "  stars_text = book['Estrellas']\n",
    "  star_map = {'One':1,'Two':2,'Three':3,'Four':4,'Five':5} # mapeo texto→número\n",
    "  stars = star_map.get(stars_text, 0) # si no existe (ej: 'No rating'), devuelve 0\n",
    "\n",
    "  description =  book['Descripcion']\n",
    "  url = book['URL']\n",
    "\n",
    "  # Insertar el autor\n",
    "  cursor.execute(\n",
    "    # ? se reemplaza por \"Neil Gaiman (nombre del autor de ejemplo)\" de forma segura\n",
    "    \"INSERT OR IGNORE INTO authors(name) VALUES(?)\", (author_name,)\n",
    "  )\n",
    "\n",
    "  # Necesito el id del autor para la FK (books / book_authors).\n",
    "  cursor.execute(\n",
    "    \"SELECT id FROM authors WHERE name = ?\", (author_name,)\n",
    "  )\n",
    "  author_id = cursor.fetchone()[0]\n",
    "\n",
    "  # Insertar los datos de los libros\n",
    "  cursor.execute(\"\"\"\n",
    "    INSERT OR IGNORE INTO books (title, price, availability, stars, description, url) VALUES (?,?,?,?,?,?)\n",
    "  \"\"\", (title, price, availability, stars, description, url)\n",
    "  )\n",
    "\n",
    "  # Obtenemos el id del libro\n",
    "  cursor.execute(\n",
    "    \"SELECT id FROM books WHERE url = ?\", (url,)\n",
    "  )\n",
    "  book_id = cursor.fetchone()[0]\n",
    "\n",
    "  # Vinculamos libro y autor en la tabla intermedia\n",
    "  cursor.execute(\"\"\"\n",
    "    INSERT OR IGNORE INTO books_authors(book_id, author_id) VALUES (?,?)\n",
    "  \"\"\", (book_id, author_id)\n",
    "  )  \n",
    "\n",
    "# Guardamos y cerramos conexion\n",
    "conector.commit()\n",
    "cursor.close()\n",
    "conector.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
