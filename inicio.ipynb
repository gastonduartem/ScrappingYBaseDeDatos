{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1381b72",
   "metadata": {},
   "source": [
    "¬øQu√© Es el Web Scraping?\n",
    "El web scraping es un conjunto de pr√°cticas utilizadas para extraer autom√°ticamente ‚Äî o ¬´scrapear¬ª ‚Äî datos de la web.\n",
    "\n",
    "El web scraping se refiere al proceso de extracci√≥n de contenidos y datos de sitios web mediante software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202bdac",
   "metadata": {},
   "source": [
    "Este script realiza web scraping de la p√°gina de un libro espec√≠fico en el sitio \"Books to Scrape\".\n",
    "\n",
    "Primero, hacemos una petici√≥n HTTP con `requests.get(url)` para obtener el contenido HTML de la p√°gina.\n",
    "\n",
    "Luego, usamos `BeautifulSoup(response.text, 'html.parser')` para convertir ese HTML crudo en un objeto navegable.\n",
    "Esto permite buscar elementos f√°cilmente usando m√©todos como `.find()`, `.select_one()`, etc.\n",
    "\n",
    "Extraemos el t√≠tulo con `soup.find('h1').text`, el precio con `soup.select_one('.price_color').text`,  y lo limpiamos con `.replace()` para quitar caracteres no deseados (como \"√Ç\").\n",
    "\n",
    "Para la disponibilidad, usamos `.select_one('.availability').text.strip()` para eliminar espacios en blanco.\n",
    "\n",
    "La descripci√≥n del libro est√° justo despu√©s del div con id `product_description`, por eso usamos `.find_next_sibling('p')`.\n",
    "\n",
    "Finalmente, las estrellas est√°n codificadas como una clase CSS en la etiqueta <p>, como por ejemplo: `<p class=\"star-rating Three\">.`\n",
    "\n",
    "Al usar `tag['class'][1]`, obtenemos el texto que indica la cantidad de estrellas (por ejemplo, \"Three\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db650b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√≠tulo: A Light in the Attic\n",
      "Precio: ¬£51.77\n",
      "Disponibilidad: In stock (22 available)\n",
      "Descripci√≥n: It's hard to imagine a world without A Light in the Attic. T...\n",
      "Estrellas: Three\n"
     ]
    }
   ],
   "source": [
    "# Extraer la informaci√≥n de un solo libro\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup #Averiguar bien\n",
    "\n",
    "# URL del libro\n",
    "url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "\n",
    "# Hacemos la petici√≥n\n",
    "response = requests.get(url)\n",
    "\n",
    "# BeautifulSoup(response.text, 'html.parser') transforma el HTML crudo en un objeto navegable.\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Obtenemos el t√≠tulo\n",
    "title = soup.find('h1').text\n",
    "\n",
    "# .text te devuelve el texto visible dentro de una etiqueta HTML.\n",
    "# La gente usa .text sin saber qu√© devuelve la etiqueta. Si la etiqueta no tiene texto visible (ej: <img> o <meta>), .text te da vac√≠o.\n",
    "\n",
    "# Obtenemos precio\n",
    "price = soup.select_one('.price_color').text\n",
    "price = price.replace('√Ç', '')  # Limpia cualquier car√°cter raro que aparezca\n",
    "\n",
    "# Obtenemos disponibilidad\n",
    "availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "# Descripci√≥n (viene en el siguiente <p> despu√©s del div con id 'product_description')\n",
    "desc_tag = soup.find('div', id='product_description')\n",
    "# Usamos `.find_next_sibling('p')`, que busca el **pr√≥ximo hermano (sibling)** que sea una etiqueta <p>.\n",
    "description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripci√≥n\"\n",
    "\n",
    "# Las estrellas est√°n codificadas en la clase CSS de un <p>\n",
    "# Ese guion bajo es una convenci√≥n para evitar conflicto. Internamente, BeautifulSoup ya sabe que class_ se refiere al atributo class del HTML\n",
    "# Al hacer tag['class'], obtenemos una lista como ['star-rating', 'Three'], y el √≠ndice [1] nos da la cantidad de estrellas en texto.\n",
    "star_tag = soup.find('p', class_='star-rating')\n",
    "stars = star_tag['class'][1]\n",
    "\n",
    "# Imprimimos\n",
    "print(f\"T√≠tulo: {title}\")\n",
    "print(f\"Precio: {price}\")\n",
    "print(f\"Disponibilidad: {availability}\")\n",
    "print(f\"Descripci√≥n: {description[:60]}...\")\n",
    "print(f\"Estrellas: {stars}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c25ea1",
   "metadata": {},
   "source": [
    "Scrapeo de varias p√°ginas\n",
    "\n",
    "Que hacen:\n",
    "\n",
    "1. requests.get(url) hace una petici√≥n HTTP a la URL y devuelve la respuesta con el HTML.\n",
    "\n",
    "2. BeautifulSoup transforma el texto HTML en un objeto para buscar etiquetas f√°cilmente.\n",
    "\n",
    "3. .find() busca la primera etiqueta que cumple la condici√≥n.\n",
    "\n",
    "4. .select() busca todas las etiquetas que cumplen un selector CSS (devuelve lista).\n",
    "\n",
    "5. .select_one() busca el primer elemento que cumple el selector CSS (devuelve un tag).\n",
    "\n",
    "6. En las URLs, a veces hay rutas relativas con \"../\", hay que limpiarlas para crear URLs absolutas.\n",
    "\n",
    "7. time.sleep(0.1) hace que el programa espere 0.1 segundos para no saturar el servidor y evitar bloqueos.\n",
    "\n",
    "8. El bucle while True se usa para seguir scrapeando p√°ginas hasta que no haya m√°s p√°ginas siguientes.\n",
    "\n",
    "9. Las estrellas del libro est√°n en la clase CSS 'star-rating X', donde X es la cantidad en texto (One, Two, Three, etc).\n",
    "\n",
    "10. Guardamos los datos en una lista de diccionarios para luego exportarlos o procesarlos f√°cilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3859dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-last-mile-amos-decker-2_754/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-last-mile-amos-decker-2_754/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d547080>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d9bbce0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/hide-away-eve-duncan-20_620/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/hide-away-eve-duncan-20_620/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d8b9190>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-widow_609/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-widow_609/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d60af60>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html\n",
      "Total de libros scrapeados: 32\n",
      "[{'Titulo': 'Sharp Objects', 'Precio': '¬£47.82', 'Disponibilidad': 'In stock (20 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/sharp-objects_997/index.html'}, {'Titulo': 'In a Dark, Dark Wood', 'Precio': '¬£19.63', 'Disponibilidad': 'In stock (18 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/in-a-dark-dark-wood_963/index.html'}, {'Titulo': 'The Past Never Ends', 'Precio': '¬£56.50', 'Disponibilidad': 'In stock (16 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html'}, {'Titulo': 'A Murder in Time', 'Precio': '¬£16.64', 'Disponibilidad': 'In stock (16 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/a-murder-in-time_877/index.html'}, {'Titulo': 'The Murder of Roger Ackroyd (Hercule Poirot #4)', 'Precio': '¬£44.10', 'Disponibilidad': 'In stock (15 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html'}, None, {'Titulo': 'That Darkness (Gardiner and Renner #1)', 'Precio': '¬£13.92', 'Disponibilidad': 'In stock (14 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/that-darkness-gardiner-and-renner-1_743/index.html'}, {'Titulo': 'Tastes Like Fear (DI Marnie Rome #3)', 'Precio': '¬£10.69', 'Disponibilidad': 'In stock (14 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/tastes-like-fear-di-marnie-rome-3_742/index.html'}, {'Titulo': 'A Time of Torment (Charlie Parker #14)', 'Precio': '¬£48.35', 'Disponibilidad': 'In stock (14 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/a-time-of-torment-charlie-parker-14_657/index.html'}, None, {'Titulo': 'Poisonous (Max Revere Novels #3)', 'Precio': '¬£26.80', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/poisonous-max-revere-novels-3_627/index.html'}, {'Titulo': 'Murder at the 42nd Street Library (Raymond Ambler #1)', 'Precio': '¬£54.36', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/murder-at-the-42nd-street-library-raymond-ambler-1_624/index.html'}, {'Titulo': 'Most Wanted', 'Precio': '¬£35.28', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/most-wanted_623/index.html'}, None, {'Titulo': 'Boar Island (Anna Pigeon #19)', 'Precio': '¬£59.48', 'Disponibilidad': 'In stock (12 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/boar-island-anna-pigeon-19_613/index.html'}, None, {'Titulo': 'Playing with Fire', 'Precio': '¬£13.71', 'Disponibilidad': 'In stock (11 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/playing-with-fire_602/index.html'}, {'Titulo': 'What Happened on Beale Street (Secrets of the South Mysteries #2)', 'Precio': '¬£25.37', 'Disponibilidad': 'In stock (7 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html'}, {'Titulo': \"The Bachelor Girl's Guide to Murder (Herringford and Watts Mysteries #1)\", 'Precio': '¬£52.30', 'Disponibilidad': 'In stock (7 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/the-bachelor-girls-guide-to-murder-herringford-and-watts-mysteries-1_491/index.html'}, {'Titulo': 'Delivering the Truth (Quaker Midwife Mystery #1)', 'Precio': '¬£20.89', 'Disponibilidad': 'In stock (7 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/delivering-the-truth-quaker-midwife-mystery-1_464/index.html'}, {'Titulo': 'The Mysterious Affair at Styles (Hercule Poirot #1)', 'Precio': '¬£24.80', 'Disponibilidad': 'In stock (6 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-mysterious-affair-at-styles-hercule-poirot-1_452/index.html'}, {'Titulo': 'In the Woods (Dublin Murder Squad #1)', 'Precio': '¬£38.38', 'Disponibilidad': 'In stock (6 available)', 'Estrellas': 'Two', 'URL': 'https://books.toscrape.com/catalogue/in-the-woods-dublin-murder-squad-1_433/index.html'}, {'Titulo': 'The Silkworm (Cormoran Strike #2)', 'Precio': '¬£23.05', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/the-silkworm-cormoran-strike-2_280/index.html'}, {'Titulo': 'The Exiled', 'Precio': '¬£43.45', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/the-exiled_247/index.html'}, {'Titulo': \"The Cuckoo's Calling (Cormoran Strike #1)\", 'Precio': '¬£19.21', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/the-cuckoos-calling-cormoran-strike-1_239/index.html'}, {'Titulo': 'Extreme Prey (Lucas Davenport #26)', 'Precio': '¬£25.40', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/extreme-prey-lucas-davenport-26_154/index.html'}, {'Titulo': 'Career of Evil (Cormoran Strike #3)', 'Precio': '¬£24.72', 'Disponibilidad': 'In stock (3 available)', 'Estrellas': 'Two', 'URL': 'https://books.toscrape.com/catalogue/career-of-evil-cormoran-strike-3_137/index.html'}, {'Titulo': \"The No. 1 Ladies' Detective Agency (No. 1 Ladies' Detective Agency #1)\", 'Precio': '¬£57.70', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Four', 'URL': 'https://books.toscrape.com/catalogue/the-no-1-ladies-detective-agency-no-1-ladies-detective-agency-1_76/index.html'}, {'Titulo': 'The Girl You Lost', 'Precio': '¬£12.29', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Five', 'URL': 'https://books.toscrape.com/catalogue/the-girl-you-lost_66/index.html'}, {'Titulo': 'The Girl In The Ice (DCI Erika Foster #1)', 'Precio': '¬£15.85', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/the-girl-in-the-ice-dci-erika-foster-1_65/index.html'}, {'Titulo': 'Blood Defense (Samantha Brinkman #1)', 'Precio': '¬£20.30', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'Three', 'URL': 'https://books.toscrape.com/catalogue/blood-defense-samantha-brinkman-1_8/index.html'}, {'Titulo': \"1st to Die (Women's Murder Club #1)\", 'Precio': '¬£53.98', 'Disponibilidad': 'In stock (1 available)', 'Estrellas': 'One', 'URL': 'https://books.toscrape.com/catalogue/1st-to-die-womens-murder-club-1_2/index.html'}]\n"
     ]
    }
   ],
   "source": [
    "# Extraer la informaci√≥n de una categor√≠a\n",
    "\n",
    "import requests # Para hacer peticiones HTTPS y obtener el contenido de las p√°ginas\n",
    "from bs4 import BeautifulSoup # Para parsear (analizar) el contenido de la p√°gina web\n",
    "import time # Para pausar entre peticiones y no sobrecargar el servidor\n",
    "\n",
    "base_url = 'https://books.toscrape.com/catalogue/'\n",
    "\n",
    "category_url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
    "\n",
    "# Lista donde se guardaran los datos de todos los libros\n",
    "all_books = []\n",
    "\n",
    "# Funci√≥n scrape_book, toma la url de un libro especifico y devuelve los datos scrapeados en forma de diccionario\n",
    "def scrape_book(url):\n",
    "\n",
    "  # Hacemos la petici√≥n\n",
    "  try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "  except requests.RequestException as e:\n",
    "    print(f\"‚ùå Error al acceder a {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "  # BeautifulSoup(response.text, 'html.parser') transforma el HTML crudo en un objeto navegable.\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Obtenemos el t√≠tulo\n",
    "  title = soup.find('h1').text\n",
    "\n",
    "  # Obtenemos precio\n",
    "  price = soup.select_one('.price_color').text\n",
    "  price = price.replace('√Ç', '')  # Limpia cualquier car√°cter raro que aparezca\n",
    "\n",
    "  # Obtenemos disponibilidad\n",
    "  availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "  # Descripci√≥n (viene en el siguiente <p> despu√©s del div con id 'product_description')\n",
    "  desc_tag = soup.find('div', id='product_description')\n",
    "  # Usamos `.find_next_sibling('p')`, que busca el **pr√≥ximo hermano (sibling)** que sea una etiqueta <p>.\n",
    "  description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripci√≥n\"\n",
    "\n",
    "  # Las estrellas est√°n codificadas en la clase CSS de un <p>\n",
    "  # Ese guion bajo es una convenci√≥n para evitar conflicto. Internamente, BeautifulSoup ya sabe que class_ se refiere al atributo class del HTML\n",
    "  # Al hacer tag['class'], obtenemos una lista como ['star-rating', 'Three'], y el √≠ndice [1] nos da la cantidad de estrellas en texto.\n",
    "  star_tag = soup.find('p', class_='star-rating')\n",
    "  stars = star_tag['class'][1]\n",
    "\n",
    "  return {\n",
    "    'Titulo': title,\n",
    "    'Precio': price,\n",
    "    'Disponibilidad': availability,\n",
    "    'Estrellas': stars,\n",
    "    'URL': url\n",
    "  }\n",
    "\n",
    "# Funcion scrape_category, toma la url de una categoria, recorre todas sus paginas y scrapea todos los libros que encuentra\n",
    "def scrape_category(url):\n",
    "  while True:\n",
    "    print(f\"Scrapeando pagina: {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Seleccionamos todos los libros en la pagina actual de esa categor√≠a\n",
    "    book_articles = soup.select('article.product_pod')\n",
    "\n",
    "    for book in book_articles:\n",
    "\n",
    "      # Obtenemos el href del libro\n",
    "      relative_url = book.find('h3').find('a')['href']\n",
    "\n",
    "      # relative_url trae la url completa, entonces la limpiamos para despu√©s concatenarla a base_url\n",
    "      book_url = base_url + relative_url.replace('../../../', '')\n",
    "\n",
    "      # Scrapeamos el libro\n",
    "      book_data = scrape_book(book_url)\n",
    "\n",
    "      # Guardamos el resultado\n",
    "      all_books.append(book_data)\n",
    "\n",
    "      # Pausa para evitar sobrecarga\n",
    "      time.sleep(10)\n",
    "\n",
    "    # Verificamos si hay un boton next para seguir scrapeando la sgte pagina\n",
    "    # Usa un selector CSS (li.next > a) para encontrar el link de la siguiente p√°gina. Si existe: next_button va a tener un tag <a>.\n",
    "    next_button = soup.select_one('li.next > a')\n",
    "    if next_button:\n",
    "      next_page_url = next_button['href']\n",
    "      # rsplit(separador, cantidad) divide un string desde el final, y solo hace la cantidad de cortes que vos le indiques.\n",
    "      url = url.rsplit('/', 1)[0] + '/' + next_page_url\n",
    "    else:\n",
    "      break\n",
    "\n",
    "scrape_category(category_url)\n",
    "\n",
    "print(f\"Total de libros scrapeados: {len(all_books)}\")\n",
    "print(all_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad444aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Se encontraron 50 categorias. \n",
      "\n",
      "\n",
      " Scrapeando categoria: Travel -> https://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "\n",
      " Scrapeando categoria: Mystery -> https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-past-never-ends_942/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-past-never-ends_942/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d5832f0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-murder-of-roger-ackroyd-hercule-poirot-4_852/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc20710>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-last-mile-amos-decker-2_754/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-last-mile-amos-decker-2_754/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853eb880b0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/a-study-in-scarlet-sherlock-holmes-1_656/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc202f0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/most-wanted_623/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/most-wanted_623/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dd50b90>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/what-happened-on-beale-street-secrets-of-the-south-mysteries-2_506/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853ebc9910>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-bachelor-girls-guide-to-murder-herringford-and-watts-mysteries-1_491/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-bachelor-girls-guide-to-murder-herringford-and-watts-mysteries-1_491/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc468a0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/delivering-the-truth-quaker-midwife-mystery-1_464/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/delivering-the-truth-quaker-midwife-mystery-1_464/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dc474a0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/in-the-woods-dublin-murder-squad-1_433/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/in-the-woods-dublin-murder-squad-1_433/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d582ab0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-exiled_247/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-exiled_247/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853e07e9c0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-cuckoos-calling-cormoran-strike-1_239/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-cuckoos-calling-cormoran-strike-1_239/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853dd502c0>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/extreme-prey-lucas-davenport-26_154/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/extreme-prey-lucas-davenport-26_154/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853e07f320>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n",
      "\n",
      " Scrapeando categoria: Historical Fiction -> https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html\n",
      "Scrapeando pagina: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html\n",
      "‚ùå Error al acceder a https://books.toscrape.com/catalogue/the-constant-princess-the-tudor-court-1_493/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/the-constant-princess-the-tudor-court-1_493/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7d853d92bb30>, 'Connection to books.toscrape.com timed out. (connect timeout=10)'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Scrapeando categoria: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory.text.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m     scrape_category(category_url)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mscrape_all_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal de libros scrapeados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_books)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_books)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mscrape_all_categories\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    107\u001b[39m category_url = base_site + relative_url\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Scrapeando categoria: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory.text.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43mscrape_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mscrape_category\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     77\u001b[39m   all_books.append(book_data)\n\u001b[32m     79\u001b[39m   \u001b[38;5;66;03m# Pausa para evitar sobrecarga\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m   \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Verificamos si hay un boton next para seguir scrapeando la sgte pagina\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Usa un selector CSS (li.next > a) para encontrar el link de la siguiente p√°gina. Si existe: next_button va a tener un tag <a>.\u001b[39;00m\n\u001b[32m     84\u001b[39m next_button = soup.select_one(\u001b[33m'\u001b[39m\u001b[33mli.next > a\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Extraer informaci√≥n de toda la p√°gina\n",
    "\n",
    "import requests # Para hacer peticiones HTTPS y obtener el contenido de las p√°ginas\n",
    "from bs4 import BeautifulSoup # Para parsear (analizar) el contenido de la p√°gina web\n",
    "import time # Para pausar entre peticiones y no sobrecargar el servidor\n",
    "\n",
    "base_url = 'https://books.toscrape.com/catalogue/'\n",
    "\n",
    "# Lista donde se guardaran los datos de todos los libros\n",
    "all_books = []\n",
    "\n",
    "# Funci√≥n scrape_book, toma la url de un libro especifico y devuelve los datos scrapeados en forma de diccionario\n",
    "def scrape_book(url):\n",
    "\n",
    "  # Hacemos la petici√≥n\n",
    "  try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "  except requests.RequestException as e:\n",
    "    print(f\"‚ùå Error al acceder a {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "  # BeautifulSoup(response.text, 'html.parser') transforma el HTML crudo en un objeto navegable.\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Obtenemos el t√≠tulo\n",
    "  title = soup.find('h1').text\n",
    "\n",
    "  # Obtenemos precio\n",
    "  price = soup.select_one('.price_color').text\n",
    "  price = price.replace('√Ç', '')  # Limpia cualquier car√°cter raro que aparezca\n",
    "\n",
    "  # Obtenemos disponibilidad\n",
    "  availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "  # Descripci√≥n (viene en el siguiente <p> despu√©s del div con id 'product_description')\n",
    "  desc_tag = soup.find('div', id='product_description')\n",
    "  # Usamos `.find_next_sibling('p')`, que busca el **pr√≥ximo hermano (sibling)** que sea una etiqueta <p>.\n",
    "  description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripci√≥n\"\n",
    "\n",
    "  # Las estrellas est√°n codificadas en la clase CSS de un <p>\n",
    "  # Ese guion bajo es una convenci√≥n para evitar conflicto. Internamente, BeautifulSoup ya sabe que class_ se refiere al atributo class del HTML\n",
    "  # Al hacer tag['class'], obtenemos una lista como ['star-rating', 'Three'], y el √≠ndice [1] nos da la cantidad de estrellas en texto.\n",
    "  star_tag = soup.find('p', class_='star-rating')\n",
    "  stars = star_tag['class'][1]\n",
    "\n",
    "  return {\n",
    "    'Titulo': title,\n",
    "    'Precio': price,\n",
    "    'Disponibilidad': availability,\n",
    "    'Estrellas': stars,\n",
    "    'URL': url\n",
    "  }\n",
    "\n",
    "# Funcion scrape_category, toma la url de una categoria, recorre todas sus paginas y scrapea todos los libros que encuentra\n",
    "def scrape_category(url):\n",
    "  while True:\n",
    "    print(f\"Scrapeando pagina: {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Seleccionamos todos los libros en la pagina actual de esa categor√≠a\n",
    "    book_articles = soup.select('article.product_pod')\n",
    "\n",
    "    for book in book_articles:\n",
    "\n",
    "      # Obtenemos el href del libro\n",
    "      relative_url = book.find('h3').find('a')['href']\n",
    "\n",
    "      # relative_url trae la url completa, entonces la limpiamos para despu√©s concatenarla a base_url\n",
    "      book_url = base_url + relative_url.replace('../../../', '')\n",
    "\n",
    "      # Scrapeamos el libro\n",
    "      book_data = scrape_book(book_url)\n",
    "\n",
    "      # Guardamos el resultado\n",
    "      all_books.append(book_data)\n",
    "\n",
    "      # Pausa para evitar sobrecarga\n",
    "      time.sleep(10)\n",
    "\n",
    "    # Verificamos si hay un boton next para seguir scrapeando la sgte pagina\n",
    "    # Usa un selector CSS (li.next > a) para encontrar el link de la siguiente p√°gina. Si existe: next_button va a tener un tag <a>.\n",
    "    next_button = soup.select_one('li.next > a')\n",
    "    if next_button:\n",
    "      next_page_url = next_button['href']\n",
    "      # rsplit(separador, cantidad) divide un string desde el final, y solo hace la cantidad de cortes que vos le indiques.\n",
    "      url = url.rsplit('/', 1)[0] + '/' + next_page_url\n",
    "    else:\n",
    "      break\n",
    "\n",
    "def scrape_all_categories():\n",
    "\n",
    "  home_url = 'https://books.toscrape.com/index.html'\n",
    "  base_site = 'https://books.toscrape.com/'\n",
    "\n",
    "  response = requests.get(home_url)\n",
    "  response.raise_for_status()\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  category_links = soup.select('div.side_categories ul li ul li a')\n",
    "\n",
    "  print(f\"\\n Se encontraron {len(category_links)} categorias. \\n\")\n",
    "\n",
    "  for category in category_links:\n",
    "    relative_url = category['href']\n",
    "    category_url = base_site + relative_url\n",
    "\n",
    "    print(f\"\\n Scrapeando categoria: {category.text.strip()} -> {category_url}\")\n",
    "\n",
    "    scrape_category(category_url)\n",
    "\n",
    "scrape_all_categories()\n",
    "\n",
    "print(f\"Total de libros scrapeados: {len(all_books)}\")\n",
    "print(all_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842091e",
   "metadata": {},
   "source": [
    "Para poder pasar los datos a un CSV\n",
    "\n",
    "Un archivo CSV (valores separados por comas) es un archivo de texto que tiene un formato espec√≠fico que permite guardar los datos en un formato de tabla estructurada.\n",
    "\n",
    "with ... as f: üëâ Context manager: abre y cierra el archivo solo, incluso si explota algo. f es el manejador.\n",
    "\n",
    "open(path, ...) üëâ ruta del archivo. Si no existe la carpeta, revienta con FileNotFoundError.\n",
    "\n",
    "'w' üëâ write: crea el archivo o sobrescribe si ya existe.\n",
    "\n",
    "newline='' üëâ especial para CSV en Windows: evita que se inserten l√≠neas en blanco entre filas.\n",
    "\n",
    "encoding='utf-8' üëâ guarda acentos/√±/emoji sin romperse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-1.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-2.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-3.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-4.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-5.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-6.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-7.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-8.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-9.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-10.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-11.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-12.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-13.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-14.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-15.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-16.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-17.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-18.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-19.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-20.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-21.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-22.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-23.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-24.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-25.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-26.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-27.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-28.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-29.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-30.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-31.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-32.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-33.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-34.html\n",
      "Scrapeando p√°gina: https://books.toscrape.com/catalogue/page-35.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import csv\n",
    "\n",
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "CATALOGUE_URL = BASE_URL + \"catalogue/\"\n",
    "\n",
    "all_books = []\n",
    "\n",
    "def fetch_author(title):\n",
    "    try:\n",
    "        url  = f\"https://openlibrary.org/search.json?title={requests.utils.quote(title)}&limit=1\"\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "        docs = resp.json().get('docs', [])\n",
    "        if docs and docs[0].get('author_name'):\n",
    "            return docs[0]['author_name'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error al buscar autor para ¬´{title}¬ª: {e}\")\n",
    "    return \"Desconocido\"\n",
    "\n",
    "def scrape_book(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error al acceder a {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').text\n",
    "    author = fetch_author(title)\n",
    "    price = soup.select_one('.price_color').text.replace('√Ç', '')\n",
    "    availability = soup.select_one('.availability').text.strip()\n",
    "\n",
    "    desc_tag = soup.find('div', id='product_description')\n",
    "    description = desc_tag.find_next_sibling('p').text if desc_tag else \"Sin descripci√≥n\"\n",
    "\n",
    "    star_tag = soup.find('p', class_='star-rating')\n",
    "    stars = star_tag['class'][1] if star_tag else \"No rating\"\n",
    "\n",
    "    return {\n",
    "        'Titulo': title,\n",
    "        'Autor': author,\n",
    "        'Precio': price,\n",
    "        'Disponibilidad': availability,\n",
    "        'Estrellas': stars,\n",
    "        'Descripcion': description,\n",
    "        'URL': url\n",
    "    }\n",
    "\n",
    "def scrape_all_books():\n",
    "    url = CATALOGUE_URL + \"page-1.html\"\n",
    "    while True:\n",
    "        print(f\"Scrapeando p√°gina: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        books = soup.select('article.product_pod')\n",
    "        book_urls = []\n",
    "\n",
    "        for b in books:\n",
    "          # Encontramos el href del <a> dentro del <h3>\n",
    "          relative_url = b.find('h3').a['href']\n",
    "\n",
    "          # Limpiamos esa url\n",
    "          cleaned_url = relative_url.replace('../../../', '')\n",
    "\n",
    "          full_url = CATALOGUE_URL + cleaned_url\n",
    "\n",
    "          book_urls.append(full_url)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers = 15) as executor:\n",
    "          results = executor.map(scrape_book, book_urls)\n",
    "          for book_data in  results:\n",
    "            if book_data:\n",
    "              all_books.append(book_data)\n",
    "\n",
    "        # Verificamos si hay un boton next para seguir scrapeando la sgte pagina\n",
    "        # Usa un selector CSS (li.next > a) para encontrar el link de la siguiente p√°gina. Si existe: next_button va a tener un tag <a>.\n",
    "        next_button = soup.select_one('li.next > a')\n",
    "        if next_button:\n",
    "          next_page = next_button['href']\n",
    "          # rsplit(separador, cantidad) divide un string desde el final, y solo hace la cantidad de cortes que vos le indiques.\n",
    "          url = url.rsplit('/', 1)[0] + '/' + next_page\n",
    "        else:\n",
    "          break\n",
    "\n",
    "def export_books_to_csv(rows, path='books.csv'):\n",
    "    \"\"\"\n",
    "    Guarda una lista de dicts (all_books) en un CSV.\n",
    "    - Usa las claves del primer dict como encabezados.\n",
    "    - Codifica en UTF-8.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        print(\"‚ö†Ô∏è Nada para guardar (lista vac√≠a).\")\n",
    "        return\n",
    "    # escribir encabezados y filas\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
    "        headers = list(rows[0].keys())   # columnas: Titulo, Autor, Precio, etc.\n",
    "        w = csv.DictWriter(f, fieldnames=headers)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "    print(f\"‚úÖ CSV exportado: {path} ({len(rows)} filas)\")\n",
    "\n",
    "\n",
    "scrape_all_books()\n",
    "\n",
    "print(f\"\\n Total de libros scrapeados: {len(all_books)}\")\n",
    "\n",
    "export_books_to_csv(all_books)  # crea books.csv en tu carpeta actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Conectar la base de datos local 'scraping.db'\n",
    "conector = sqlite3.connect('books.db')\n",
    "cursor = conector.cursor()\n",
    "\n",
    "# Crear tabla autores\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS authors (\n",
    "               id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               name TEXT UNIQUE\n",
    "               );\n",
    "\"\"\")\n",
    "\n",
    "# Crear tabla de libros\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS books (\n",
    "               id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               title TEXT,\n",
    "               price REAL,\n",
    "               availability TEXT,\n",
    "               stars INTEGER,\n",
    "               description TEXT,\n",
    "               url TEXT UNIQUE\n",
    "               );\n",
    "\"\"\")\n",
    "\n",
    "# Creamos la tabla intermedia para lograr la relaci√≥n de muchos a muchos\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS books_authors(\n",
    "               book_id INTEGER,\n",
    "               author_id integer,\n",
    "               PRIMARY KEY (book_id, author_id),\n",
    "               FOREIGN KEY (book_id) REFERENCES books(id),\n",
    "               FOREIGN KEY (author_id) REFERENCES authors(id)\n",
    "               )\n",
    "\"\"\")\n",
    "\n",
    "# Confirmamos la creacion de las tablas\n",
    "conector.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d63fc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# üîÅ ABRIR NUEVA CONEXI√ìN + CURSOR (si cerraste antes, hay que recrearlos)\n",
    "conector = sqlite3.connect('books.db')\n",
    "conector.execute(\"PRAGMA foreign_keys = ON;\")  # que se apliquen las FKs\n",
    "cursor = conector.cursor()\n",
    "\n",
    "# Insertamos datos dentro de las tablas\n",
    "# Recorremos el diccionario y vamos guardando los datos en variables\n",
    "for book in all_books:\n",
    "  title = book['Titulo']\n",
    "  author_name = book['Autor']\n",
    "  price = float(book['Precio'].lstrip('¬£')) # Convertimos de texto a numero\n",
    "  availability = book['Disponibilidad']\n",
    "\n",
    "  stars_text = book['Estrellas']\n",
    "  star_map = {'One':1,'Two':2,'Three':3,'Four':4,'Five':5} # mapeo texto‚Üín√∫mero\n",
    "  stars = star_map.get(stars_text, 0) # si no existe (ej: 'No rating'), devuelve 0\n",
    "\n",
    "  description =  book['Descripcion']\n",
    "  url = book['URL']\n",
    "\n",
    "  # Insertar el autor\n",
    "  cursor.execute(\n",
    "    # ? se reemplaza por \"Neil Gaiman (nombre del autor de ejemplo)\" de forma segura\n",
    "    \"INSERT OR IGNORE INTO authors(name) VALUES(?)\", (author_name,)\n",
    "  )\n",
    "\n",
    "  # Necesito el id del autor para la FK (books / book_authors).\n",
    "  cursor.execute(\n",
    "    \"SELECT id FROM authors WHERE name = ?\", (author_name,)\n",
    "  )\n",
    "  author_id = cursor.fetchone()[0]\n",
    "\n",
    "  # Insertar los datos de los libros\n",
    "  cursor.execute(\"\"\"\n",
    "    INSERT OR IGNORE INTO books (title, price, availability, stars, description, url) VALUES (?,?,?,?,?,?)\n",
    "  \"\"\", (title, price, availability, stars, description, url)\n",
    "  )\n",
    "\n",
    "  # Obtenemos el id del libro\n",
    "  cursor.execute(\n",
    "    \"SELECT id FROM books WHERE url = ?\", (url,)\n",
    "  )\n",
    "  book_id = cursor.fetchone()[0]\n",
    "\n",
    "  # Vinculamos libro y autor en la tabla intermedia\n",
    "  cursor.execute(\"\"\"\n",
    "    INSERT OR IGNORE INTO books_authors(book_id, author_id) VALUES (?,?)\n",
    "  \"\"\", (book_id, author_id)\n",
    "  )  \n",
    "\n",
    "# Guardamos y cerramos conexion\n",
    "conector.commit()\n",
    "cursor.close()\n",
    "conector.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
